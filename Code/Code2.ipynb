{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Scrape the list of S&P 500 companies and their tickers\n",
    "def get_sp500_companies():\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the S&P 500 companies\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "    companies = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        company_name = cols[1].get_text(strip=True)\n",
    "        ticker = cols[0].get_text(strip=True)\n",
    "        companies.append({'Company Name': company_name, 'Ticker': ticker})\n",
    "        \n",
    "    return companies\n",
    "\n",
    "# Step 2: Fetch company data using yfinance\n",
    "def get_company_info(ticker):\n",
    "    company = yf.Ticker(ticker)\n",
    "    info = company.info\n",
    "    return {\n",
    "        'Ticker': ticker,\n",
    "        'Company Name': info.get('longName', 'N/A'),\n",
    "        'Sector': info.get('sector', 'N/A'),\n",
    "        'Industry': info.get('industry', 'N/A'),\n",
    "        'Market Cap': info.get('marketCap', 'N/A'),\n",
    "        'PE Ratio': info.get('trailingPE', 'N/A'),\n",
    "        'Dividend Yield': info.get('dividendYield', 'N/A'),\n",
    "        'Current Price': info.get('currentPrice', 'N/A'),\n",
    "        'CEO': info.get('ceo', 'N/A'),  # CEO Name\n",
    "        '52 Week High': info.get('fiftyTwoWeekHigh', 'N/A'),\n",
    "        '52 Week Low': info.get('fiftyTwoWeekLow', 'N/A')\n",
    "    }\n",
    "\n",
    "# Step 3: Fetch top 50 companies and their details, sorted by market cap\n",
    "def main():\n",
    "    companies = get_sp500_companies()\n",
    "    company_data = []\n",
    "\n",
    "    # Fetch data for each company and add market cap to the data\n",
    "    for company in companies:\n",
    "        ticker = company['Ticker']\n",
    "        company_info = get_company_info(ticker)\n",
    "        company_info['Market Cap'] = company_info.get('Market Cap', 0)  # If missing, set to 0\n",
    "        company_data.append(company_info)\n",
    "\n",
    "    # Step 4: Sort by Market Cap (descending order) and select top 50\n",
    "    sorted_company_data = sorted(company_data, key=lambda x: x['Market Cap'], reverse=True)[:150]\n",
    "\n",
    "    # Step 5: Save the data to CSV\n",
    "    df = pd.DataFrame(sorted_company_data)\n",
    "    df.to_csv('top_150_public_companies_sorted.csv', index=False)\n",
    "    print(\"Data saved to 'top_150_public_companies_sorted.csv'.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get list of stock symbols using IEX Cloud API (Top companies)\n",
    "def get_stock_symbols(iex_api_key):\n",
    "    url = 'https://cloud.iexapis.com/stable/ref-data/symbols'\n",
    "    params = {\n",
    "        'token': iex_api_key\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Filter the data to get only active companies (remove ETFs, mutual funds, etc.)\n",
    "    active_companies = [item for item in data if item['isEnabled'] and item['type'] == 'stock']\n",
    "    \n",
    "    # Extract tickers of the active companies\n",
    "    tickers = [item['symbol'] for item in active_companies]\n",
    "    \n",
    "    return tickers[:50]  # We want the top 50, feel free to adjust\n",
    "\n",
    "# Step 2: Fetch company data using yfinance\n",
    "def get_company_info(ticker):\n",
    "    company = yf.Ticker(ticker)\n",
    "    info = company.info\n",
    "    return {\n",
    "        'Ticker': ticker,\n",
    "        'Company Name': info.get('longName', 'N/A'),\n",
    "        'Sector': info.get('sector', 'N/A'),\n",
    "        'Industry': info.get('industry', 'N/A'),\n",
    "        'Market Cap': info.get('marketCap', 'N/A'),\n",
    "        'PE Ratio': info.get('trailingPE', 'N/A'),\n",
    "        'Dividend Yield': info.get('dividendYield', 'N/A'),\n",
    "        'Current Price': info.get('currentPrice', 'N/A'),\n",
    "        'CEO': info.get('ceo', 'N/A'),\n",
    "        '52 Week High': info.get('fiftyTwoWeekHigh', 'N/A'),\n",
    "        '52 Week Low': info.get('fiftyTwoWeekLow', 'N/A')\n",
    "    }\n",
    "\n",
    "# Step 3: Fetch top 50 companies and their details, sorted by market cap\n",
    "def main():\n",
    "    # Your IEX Cloud API key (replace with your own)\n",
    "    iex_api_key = 'your_iex_api_key_here'\n",
    "\n",
    "    # Get the list of stock tickers for top companies\n",
    "    tickers = get_stock_symbols(iex_api_key)\n",
    "    \n",
    "    company_data = []\n",
    "\n",
    "    # Fetch data for each company and add market cap to the data\n",
    "    for ticker in tickers:\n",
    "        company_info = get_company_info(ticker)\n",
    "        company_info['Market Cap'] = company_info.get('Market Cap', 0)  # If missing, set to 0\n",
    "        company_data.append(company_info)\n",
    "\n",
    "    # Step 4: Sort by Market Cap (descending order) and select top 50\n",
    "    sorted_company_data = sorted(company_data, key=lambda x: x['Market Cap'], reverse=True)[:50]\n",
    "\n",
    "    # Step 5: Save the data to CSV\n",
    "    df = pd.DataFrame(sorted_company_data)\n",
    "    df.to_csv('top_50_public_companies_sorted_api.csv', index=False)\n",
    "    print(\"Data saved to 'top_50_public_companies_sorted_api.csv'.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivanimadan/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Extract relevant information\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m businesses \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m women_owned_companies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m business \u001b[38;5;129;01min\u001b[39;00m businesses:\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the Crunchbase API URL and API Key\n",
    "api_url = 'https://api.crunchbase.com/v3.1/organizations'\n",
    "api_key = 'your_api_key_here'\n",
    "\n",
    "# Example query for fetching women-led businesses (you can customize the query)\n",
    "params = {\n",
    "    'user_key': api_key,\n",
    "    'query': 'female founders',  # This can be adjusted to reflect women-owned\n",
    "    'page': 1\n",
    "}\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(api_url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Extract relevant information\n",
    "businesses = data['data']['items']\n",
    "women_owned_companies = []\n",
    "for business in businesses:\n",
    "    name = business['name']\n",
    "    sector = business['category_group_list']\n",
    "    funding = business['funding_total_usd']\n",
    "    employees = business['number_of_employees']\n",
    "    ceo = business.get('ceo', 'N/A')\n",
    "    gender = \"Female\" if \"woman\" in ceo.lower() else \"Unknown\"\n",
    "    women_owned_companies.append({\n",
    "        'Name': name,\n",
    "        'Sector': sector,\n",
    "        'Funding': funding,\n",
    "        'Employees': employees,\n",
    "        'CEO': ceo,\n",
    "        'Gender': gender\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "import pandas as pd\n",
    "df_women = pd.DataFrame(women_owned_companies)\n",
    "df_women.to_csv('women_Companies.csv', index=False)\n",
    "print(\"Women-owned companies saved to 'women_Companies.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"status\": 401,\n",
      "    \"code\": \"LA401\",\n",
      "    \"message\": \"Unauthorized user_key\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the Crunchbase API URL and API Key\n",
    "api_url = 'https://api.crunchbase.com/v3.1/organizations'\n",
    "api_key = '7130f22d328d4554bc00013a3aff6b6c'\n",
    "\n",
    "# Example query for fetching women-led businesses (you can customize the query)\n",
    "params = {\n",
    "    'user_key': api_key,\n",
    "    'query': 'female founders',  # This can be adjusted to reflect women-owned\n",
    "    'page': 1\n",
    "}\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(api_url, params=params)\n",
    "\n",
    "# Check the structure of the response\n",
    "data = response.json()\n",
    "\n",
    "# Print the structure of the returned JSON data\n",
    "print(json.dumps(data, indent=2))  # Pretty-print the JSON response\n",
    "\n",
    "# Now, depending on the structure of the data, you will modify the way you access the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: 'companies_with_ceo_and_stock_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[1;32m     51\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(company_data)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompanies_with_ceo_and_stock_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompanies_with_ceo_and_stock_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'companies_with_ceo_and_stock_data.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Alpha Vantage API Key\n",
    "alpha_vantage_api_key = \"PP740KVKIVI10QZG\"\n",
    "\n",
    "# Example list of tickers (you can scrape this list from Yahoo Finance or any other source)\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"TSLA\"]\n",
    "\n",
    "# Function to get financial data from Alpha Vantage (using the Time Series API as an example)\n",
    "def get_alpha_vantage_data(ticker):\n",
    "    url = f'https://www.alphavantage.co/query'\n",
    "    params = {\n",
    "        'function': 'TIME_SERIES_DAILY',  # You can change this to any other function\n",
    "        'symbol': ticker,\n",
    "        'apikey': alpha_vantage_api_key\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract daily close prices (example)\n",
    "    if 'Time Series (Daily)' in data:\n",
    "        time_series = data['Time Series (Daily)']\n",
    "        daily_data = time_series[list(time_series.keys())[0]]  # Get the latest date's data\n",
    "        return daily_data['4. close']  # Example: Close price\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to get CEO data using yfinance\n",
    "def get_ceo(ticker):\n",
    "    company = yf.Ticker(ticker)\n",
    "    info = company.info\n",
    "    ceo = info.get('ceo', 'N/A')\n",
    "    return ceo\n",
    "\n",
    "# Loop through tickers and collect data\n",
    "company_data = []\n",
    "for ticker in tickers:\n",
    "    ceo = get_ceo(ticker)\n",
    "    close_price = get_alpha_vantage_data(ticker)\n",
    "    \n",
    "    company_info = {\n",
    "        'Ticker': ticker,\n",
    "        'CEO': ceo,\n",
    "        'Close Price (Alpha Vantage)': close_price\n",
    "    }\n",
    "    company_data.append(company_info)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(company_data)\n",
    "df.to_csv('companies_with_ceo_and_stock_data.csv', index=False)\n",
    "print(\"Data saved to 'companies_with_ceo_and_stock_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping company with invalid or missing ticker: JPMorganChase\n",
      "Fetching data for Berkshire Hathaway (BRK-B)...\n",
      "Skipping company with invalid or missing ticker: Saudi Arabian Oil Company (Saudi Aramco)\n",
      "Fetching data for ICBC (1398.HK)...\n",
      "Fetching data for Bank of America (BAC)...\n",
      "Fetching data for Amazon (AMZN)...\n",
      "Fetching data for China Construction Bank (601939.SS)...\n",
      "Fetching data for Microsoft (MSFT)...\n",
      "Fetching data for Agricultural Bank of China (601288.SS)...\n",
      "Fetching data for Alphabet (GOOG)...\n",
      "Fetching data for Toyota Motor (TM)...\n",
      "Fetching data for Apple (AAPL)...\n",
      "Fetching data for Bank of China (3988.HK)...\n",
      "Skipping company with invalid or missing ticker: ExxonMobil\n",
      "Fetching data for HSBC Holdings (HSBA.L)...\n",
      "Fetching data for Wells Fargo (WFC)...\n",
      "Fetching data for Shell Plc (SHEL)...\n",
      "Fetching data for PetroChina (0857.HK)...\n",
      "Fetching data for UnitedHealth Group (UNH)...\n",
      "Fetching data for Walmart (WMT)...\n",
      "Fetching data for Samsung Electronics (005930.KS)...\n",
      "Fetching data for Chevron (CVX)...\n",
      "Fetching data for Goldman Sachs Group (GS)...\n",
      "Fetching data for Meta Platforms (META)...\n",
      "Fetching data for TotalEnergies (TTE.PA)...\n",
      "Fetching data for Morgan Stanley (MS)...\n",
      "Fetching data for RBC (RBC)...\n",
      "Fetching data for Citigroup (C)...\n",
      "Fetching data for Ping An Insurance Group (2318.HK)...\n",
      "Fetching data for China Mobile (0941.HK)...\n",
      "Fetching data for Allianz (DI7R.F)...\n",
      "Fetching data for BNP Paribas (BNP.PA)...\n",
      "Fetching data for Verizon Communications (VZ)...\n",
      "Fetching data for China Merchants Bank (3968.HK)...\n",
      "Fetching data for UBS (UBS)...\n",
      "Fetching data for Santander (SAN)...\n",
      "Fetching data for AT&T (T)...\n",
      "Fetching data for Tencent Holdings (TCEHY)...\n",
      "Fetching data for Mitsubishi UFJ Financial (MUFG)...\n",
      "Fetching data for Comcast (CMCSA)...\n",
      "Fetching data for Alibaba Group (BABA)...\n",
      "Fetching data for Sinopec (600028.SS)...\n",
      "Skipping company with invalid or missing ticker: Volkswagen Group\n",
      "Skipping company with invalid or missing ticker: TD Bank Group\n",
      "Fetching data for Johnson & Johnson (JNJ)...\n",
      "Fetching data for Taiwan Semiconductor (TSM)...\n",
      "Fetching data for BP (BP)...\n",
      "Skipping company with invalid or missing ticker: LVMH Moët Hennessy Louis Vuitton\n",
      "Fetching data for Reliance Industries (RELIANCE.NS)...\n",
      "Fetching data for Nestlé (NESN.SW)...\n",
      "Skipping company with invalid or missing ticker: AXA Group\n",
      "Fetching data for Mercedes-Benz Group (MBG.DE)...\n",
      "Fetching data for American Express (AXP)...\n",
      "Fetching data for Petrobras (PBR)...\n",
      "Fetching data for State Bank of India (SBIN.NS)...\n",
      "Fetching data for Bank of Communications (3328.HK)...\n",
      "Skipping company with invalid or missing ticker: Postal Savings Bank Of China (PSBC)\n",
      "Fetching data for Tesla (TSLA)...\n",
      "Fetching data for Procter & Gamble (PG)...\n",
      "Fetching data for Stellantis (STLA)...\n",
      "Skipping company with invalid or missing ticker: BMW Group\n",
      "Fetching data for Deutsche Telekom (DTE.DE)...\n",
      "Fetching data for Sumitomo Mitsui Financial (SMFG)...\n",
      "Fetching data for CVS Health (CVS)...\n",
      "Skipping company with invalid or missing ticker: HDFC Bank\n",
      "Skipping company with invalid or missing ticker: BBVA-Banco Bilbao Vizcaya\n",
      "Fetching data for Sony (SONY)...\n",
      "Fetching data for Siemens (SIE.DE)...\n",
      "Fetching data for Commonwealth Bank (CBA.AX)...\n",
      "Fetching data for Life Insurance Corp. of India (LICI.BO)...\n",
      "Fetching data for General Motors (GM)...\n",
      "Fetching data for Nippon Telegraph & Tel (NITT.VI)...\n",
      "Fetching data for Credit Agricole (ACA.VI)...\n",
      "Fetching data for Broadcom (AVGO)...\n",
      "Fetching data for PepsiCo (PEP)...\n",
      "Fetching data for The Home Depot (HD)...\n",
      "Fetching data for Oracle (ORCL)...\n",
      "Fetching data for Mitsubishi (MUFG)...\n",
      "Fetching data for Roche Holding (ROG.SW)...\n",
      "Fetching data for Equinor (EQNR)...\n",
      "Fetching data for Elevance Health (ELV)...\n",
      "Fetching data for CNOOC (600938.SS)...\n",
      "Fetching data for Industrial Bank (601166.SS)...\n",
      "Skipping company with invalid or missing ticker: Intesa Sanpaolo Assicura\n",
      "Fetching data for IBM (IBM)...\n",
      "Fetching data for Chubb (CB)...\n",
      "Fetching data for Itaú Unibanco Holding (ITUB)...\n",
      "Fetching data for Anheuser-Busch InBev (BUD)...\n",
      "Fetching data for AbbVie (ABBV)...\n",
      "Fetching data for Cisco Systems (CSCO)...\n",
      "Fetching data for Honda Motor (HMC)...\n",
      "Fetching data for Bank of Nova Scotia (BNS.TO)...\n",
      "Fetching data for Hyundai Motor (HYMTF)...\n",
      "Fetching data for Enel (E19.F)...\n",
      "Fetching data for Cigna (CI)...\n",
      "Fetching data for Costco Wholesale (COST)...\n",
      "Fetching data for Munich Re (DE000SL0HX99.SG)...\n",
      "Fetching data for Zurich Insurance Group (ZURN.SW)...\n",
      "Fetching data for Novartis (NVS)...\n",
      "Fetching data for Coca-Cola (KO)...\n",
      "Fetching data for GE Aerospace (GE)...\n",
      "Fetching data for Caterpillar (CAT)...\n",
      "Skipping company with invalid or missing ticker: John Deere\n",
      "Fetching data for Bank of Montreal (BMO)...\n",
      "Fetching data for Mizuho Financial (MFG)...\n",
      "Fetching data for AIRBUS (AIR.PA)...\n",
      "Fetching data for Intel (INTC)...\n",
      "Fetching data for Mitsui (MTS1.MU)...\n",
      "Fetching data for ConocoPhillips (COP)...\n",
      "Fetching data for China State Construction Engineering (601668.SS)...\n",
      "Fetching data for NVIDIA (NVDA)...\n",
      "Fetching data for Rio Tinto (RIO)...\n",
      "Fetching data for CITIC (000839.SZ)...\n",
      "Fetching data for RTX (RTX)...\n",
      "Skipping company with invalid or missing ticker: Glencore International\n",
      "Fetching data for BHP Group (BHP)...\n",
      "Fetching data for China Life Insurance (2628.HK)...\n",
      "Fetching data for US Bancorp (USB.SN)...\n",
      "Fetching data for Hon Hai Precision (HNHPF)...\n",
      "Fetching data for Iberdrola (IBE.MC)...\n",
      "Fetching data for AstraZeneca (AZN)...\n",
      "Fetching data for Sanofi (SNY)...\n",
      "Fetching data for Unilever (UL)...\n",
      "Fetching data for Ford Motor (F)...\n",
      "Fetching data for Capital One (COF)...\n",
      "Skipping company with invalid or missing ticker: NAB - National Australia Bank\n",
      "Fetching data for Apollo Global Management (APO)...\n",
      "Fetching data for Visa (V)...\n",
      "Fetching data for UniCredit (UCG.MI)...\n",
      "Fetching data for Contemporary Amperex Technology (300750.SZ)...\n",
      "Fetching data for American International Group (AIG)...\n",
      "Fetching data for ANZ Group Holdings (ANZ.AX)...\n",
      "Fetching data for VINCI (SQU.BE)...\n",
      "Fetching data for China Citic Bank (601998.SS)...\n",
      "Fetching data for Thermo Fisher Scientific (TMO)...\n",
      "Fetching data for Charles Schwab (SCHW)...\n",
      "Fetching data for Progressive (PGR)...\n",
      "Fetching data for DBS (D05.SI)...\n",
      "Fetching data for Marathon Petroleum (MPC)...\n",
      "Fetching data for PNC Financial Services (PNC)...\n",
      "Skipping company with invalid or missing ticker: Generali Group\n",
      "Fetching data for ICICI Bank (IBN)...\n",
      "Fetching data for Itochu (IOC0.F)...\n",
      "Skipping company with invalid or missing ticker: Westpac Banking Group\n",
      "Fetching data for United Parcel Service (UPS)...\n",
      "Fetching data for Deutsche Bank (DB)...\n",
      "Fetching data for Canadian Imperial Bank (CM.TO)...\n",
      "Fetching data for China Shenhua Energy (601088.SS)...\n",
      "Fetching data for Tokio Marine Holdings (MH6.BE)...\n",
      "Fetching data for NextEra Energy (NEE)...\n",
      "Data saved to 'forbes_top_150_enriched_with_yahoo_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Fetch company data using yfinance (for Apple in this case)\n",
    "def get_company_info(ticker):\n",
    "    # Ensure the ticker is a string and handle any invalid or NaN values\n",
    "    if not isinstance(ticker, str):\n",
    "        print(f\"Skipping invalid ticker: {ticker}\")\n",
    "        return None\n",
    "\n",
    "    # Attempt to get data for the company\n",
    "    try:\n",
    "        company = yf.Ticker(ticker)\n",
    "        info = company.info\n",
    "        return {\n",
    "            'Ticker': ticker,\n",
    "            'Sector': info.get('sector', 'N/A'),\n",
    "            'Industry': info.get('industry', 'N/A'),\n",
    "            'Market Cap': info.get('marketCap', 0),  \n",
    "            'PE Ratio': info.get('trailingPE', 'N/A'),\n",
    "            'Dividend Yield': info.get('dividendYield', 'N/A'),\n",
    "            'Current Price': info.get('currentPrice', 'N/A'),\n",
    "            'CEO': info.get('ceo', 'N/A'),\n",
    "            '52 Week High': info.get('fiftyTwoWeekHigh', 'N/A'),\n",
    "            '52 Week Low': info.get('fiftyTwoWeekLow', 'N/A')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 2: Integrate Forbes Data with Yahoo Finance Data\n",
    "def integrate_forbes_data(file_path):\n",
    "    # Load Forbes data\n",
    "    forbes_df = pd.read_csv(file_path)\n",
    "    company_data = []\n",
    "\n",
    "    # Fetch data for each company and append it\n",
    "    for _, row in forbes_df.iterrows():\n",
    "        company_name = row['Company Name']\n",
    "        ticker = row['Ticker']\n",
    "\n",
    "        # Skip rows where the ticker is invalid or NaN\n",
    "        if pd.isna(ticker) or not isinstance(ticker, str):\n",
    "            print(f\"Skipping company with invalid or missing ticker: {company_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Fetching data for {company_name} ({ticker})...\")\n",
    "        company_info = get_company_info(ticker)\n",
    "\n",
    "        # If data is valid, append to company_data\n",
    "        if company_info:\n",
    "            # Add the Forbes columns to the Yahoo Finance data\n",
    "            company_info['Rank'] = row['Rank']\n",
    "            company_info['Company Name'] = company_name\n",
    "            company_info['Revenue'] = row['Revenue']\n",
    "            company_info['Profit'] = row['Profit']\n",
    "            company_info['Assets'] = row['Assets']\n",
    "            company_info['Market Value'] = row['Market Value']\n",
    "\n",
    "            company_data.append(company_info)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    company_data_df = pd.DataFrame(company_data)\n",
    "\n",
    "    # Step 3: Save the enriched data to a new CSV file\n",
    "    company_data_df.to_csv('forbes_top_150_enriched_with_yahoo_data.csv', index=False)\n",
    "    print(\"Data saved to 'forbes_top_150_enriched_with_yahoo_data.csv'.\")\n",
    "\n",
    "# Run the integration function\n",
    "if __name__ == '__main__':\n",
    "    integrate_forbes_data('forbes_with_ticker.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
